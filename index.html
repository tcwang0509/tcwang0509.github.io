
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<script async src="./analytics.js"></script><script type="text/javascript" src="./hidebib.js"></script>
<title>Ting-Chun Wang's Homepage</title>
<link rel="shortcut icon" href="http://cg.cs.tsinghua.edu.cn/people/~xianying/favicon.ico">
<style type="text/css">
body {
	margin-top: 30px;
	margin-bottom: 30px;
	margin-left: 100px;
	margin-right: 100px;
}
p {
	margin-top: 0px;
	margin-bottom: 0px;
}

.caption {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #990000;
}
.caption-3 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #F00;
}
.caption-4 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.content {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
}
.content a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #000;
}
.content strong a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
heading {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
  font-size: 15px;
  font-weight: 700
}
.title-small {
	font-size: 20px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #F90;
}
.title-large {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #000;
}
.margin {
	font-size: 10px;
	line-height: 10px;
}
.margin-small {
	font-size: 5px;
	line-height: 5px;
}
.margin-large {
	font-size: 16px;
	line-height: 16px;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
content a:link {
	text-decoration: none;
}
content a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: underline;
	color: #06F;
	font-family: Tahoma, Geneva, sans-serif;
}
strong a:active {
	text-decoration: underline;
	color: #06F;
}
</style>
<script async="" src="./files/analytics.js.download"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script></head>



<body>

<table border="0" width="100%">
  <tbody>

    <tr>
    <td width="185"><img src="./files/portrait.jpg" border="1" height="256"></td>
    <td width="15"></td>
    <td></td>
    <td><table border="0" width="100%">
      <tbody><tr height="10">
        <td colspan="2"></td></tr>


         <tr height="20">
        <td>
           <p class="caption">Ting-Chun Wang</p>
           <p class="content">Principal Research Scientist</p>
           <p class="content">NVIDIA</p>           
           <p class="content">Santa Clara, CA</p>
        </td>
      </tr>

      <tr height="40">
        <td><table border="0" width="100%">
          <tbody><tr height="20">
            <td width="55">
              <p class="content"><strong>Email: </strong></p></td>
            <td>
              <p class="content"> tingchunw at nvidia dot com</p></td>
          </tr>          
        </tbody></table></td>
      </tr>

      <tr height="20">
        <td>
          <p class="margin">&nbsp;</p>
          <p class="content">
          <strong><a href="https://github.com/tcwang0509/">GitHub</a></strong> | 
          <strong><a href="https://scholar.google.com/citations?user=ajXAb54AAAAJ&hl=en">Google Scholar</a></strong> </p>
        </td>
      </tr>
      <tr height="20">
        <td colspan="2"></td></tr>
    </tbody></table></td>
  </tr>
</tbody></table>
<p class="margin">&nbsp;</p>

<table border="0">
  <tbody>
    <tr>
      <td width="1200"> <p align="justify" class="content">I'm a principal research scientist at the <strong><a href="https://research.nvidia.com/labs/dir/">Deep Imagination Research (DIR)</a></strong> group at NVIDIA, working on computer vision, machine learning, and computer graphics. <br>
        I received my PhD from University of California, Berkeley, advised by Professor <a href="https://cseweb.ucsd.edu/~ravir/" target="_blank" rel="nofollow" class="caption-2">Ravi Ramamoorthi</a> and <a href="http://www.eecs.berkeley.edu/~efros/" target="_blank" rel="nofollow" class="caption-2">Alexei A. Efros</a>. <br>
        My recent research focus is on using generative models to synthesize realistic images and videos, with applications to rendering, visual manipulations and beyond. See our <strong><a href="https://www.nvidia.com/en-us/ai/cosmos/">NVIDIA Cosmos</a></strong> and <strong><a href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Picasso (Edify)</a></strong> page. <br>
        <!--<em><b>I'm looking for interns to work on GAN related problems. If you're interested, please send me your resume and a brief introduction about yourself.</strong></b></p>-->
        <br>
    </td></tr>
  </tbody>
</table>

<br>

<p id="sect-news" class="title-large">News</p>
<ul>
  <li><p class="content">Check out our <strong><a href="https://github.com/nvidia-cosmos/cosmos-predict1">Cosmos-Predict1</a></strong> and <strong><a href="https://github.com/nvidia-cosmos/cosmos-transfer1">Cosmos-Transfer1</a></strong> papers on the project website! </li>
  <li><p class="content">I am serving as an area chair at CVPR and ICLR 2025. </li>
  <!-- <li><p class="content">The JeDi paper is accepted at CVPR 2024!</li> -->
  <!-- <li><p class="content">Two papers accepted at ICCV 2023!</li> -->
  <!-- <li><p class="content">I am co-organizing the <strong><a href="https://ai4cc.net/">AI4CC workshop</a></strong> at CVPR 2023.</li> -->
  <!-- <li><p class="content">Two papers accepted at NeurIPS 2022!</li>
  <li><p class="content">Our portrait relighting work <a href="https://research.nvidia.com/labs/dir/lumos/"><strong>Lumos</strong></a> is accepted by SIGGRAPH Asia 2022! </li>
  <li><p class="content">GauGAN2 and GauGAN360 (demos for PoE-GAN) are now featured in <a href="https://www.nvidia.com/en-us/research/ai-demos/"><strong>NVIDIA AI Playground Demos</strong></a>. </li>
  <li><p class="content">Our <a href="https://blogs.nvidia.com/blog/2021/08/10/siggraph-real-time-live-demo/"><strong>digital avatar system demo</strong></a> featuring face-vid2vid won the Best-in-Show Award at SIGGRAPH RealTimeLive 2021!</li> -->
  <!-- <li><p class="content">Vid2vid-Cameo is now featured in <a href="https://www.nvidia.com/en-us/research/ai-demos/"><strong>NVIDIA AI Playground Demos</strong></a>. Check it out to play with face and gaze redirection yourself! </li>
  <li><p class="content">I was selected as an outstanding reviewer for CVPR 2021. Thank you Area Chairs!</li>
  <li><p class="content">Our <a href="https://nvlabs.github.io/face-vid2vid"><strong>face-vid2vid paper</strong></a> is accepted by CVPR 2021 as an oral presentation!</li> -->
  <!-- <li><p class="content">Check out our new paper on <a href="https://nvlabs.github.io/wc-vid2vid/"><strong>world-consistent video synthesis</strong></a>!</li>
  <li><p class="content">I am co-organizing the <strong>Mixed Precision Tutorial</strong> at ECCV 2020.</li>  -->
  <!-- <li><p class="content">Two papers accepted by NeurIPS 2019! </li> -->
  <!-- <li><p class="content">I co-organized the <a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/"><strong>Mixed Precision  Tutorial</strong></a> at ICCV 2019. </li>  -->
  <!-- <li><p class="content">I served as an area chair in WACV 2020. </li> -->
  <!-- <li><p class="content">GauGAN won <strong>Best-in-Show Award</strong> and <strong>Audience Choice Award</strong> for RealTimeLive at SIGGRAPH 2019. Check out our <a href="https://www.nvidia.com/en-us/research/ai-playground/" target="_blank" rel="nofollow" class="caption-2">online demo</a>. </li>   -->
<!--   <li><p class="content">Our SPADE paper is accepted by CVPR 2019! See our <a href="https://www.youtube.com/watch?v=p5U4NgVGAwg" target="_blank" rel="nofollow" class="caption-2">GauGAN demo</a>. </li>
  <li><p class="content">Our vid2vid paper is accepted by NeurIPS 2018! See our <a href="https://www.youtube.com/watch?v=ayPqjPekn7g" target="_blank" rel="nofollow" class="caption-2">interactive demo</a>.</li>
  <li><p class="content">Received NTECH 2018 best paper award from NVIDIA.</li>
  <li><p class="content">Received NVIDIA Pioneer Research Award from NVIDIA.</li> -->
  <!--<li><p class="content">Won 1st place in <a href="http://bdd-data.berkeley.edu/wad-2018.html" target="_blank" rel="nofollow" class="caption-2">WAD Challenge</a>, Domain Adaptation for Semantic Segmentation Competition, CVPR 2018</li>
  <li><p class="content">Presented our <a href="https://youtu.be/sIkUzmgUaxc?t=1200" target="_blank" rel="nofollow" class="caption-2">pix2pixHD</a> work at CVPR 2018.</li>
  <li><p class="content">Check out our <a href="https://twitter.com/NVIDIAAIDev/status/1009873840246280192" target="_blank" rel="nofollow" class="caption-2">face demo</a> at CVPR 2018!</li> -->
<br>
</ul>

<p id="sect-publications" class="title-large">Selected Publications</p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr> <td width="20%" valign="top"><a href="https://research.nvidia.com/labs/dir/cosmos-transfer1/images/multi_controlnet_page-0001.jpg" class="hoverZoomLink"><img src="https://research.nvidia.com/labs/dir/cosmos-transfer1/images/multi_controlnet_page-0001.jpg" alt="cosmos-transfer1" width="100%" border="1"></a>
    <td width="80%" valign="top">
    <p>
    <heading>Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control</heading></a><br>
    <p class="margin">&nbsp;</p>
    <p class="content">NVIDIA: <strong>Ting-Chun Wang</strong> (core contributor) et al.</p>
    <p class="margin-small">&nbsp;</p>
    <p class="content"><em>arXiv</em>, 2025
    <p class="margin">&nbsp;</p>
    
    <div class="paper" id="cosmos-transfer1">    
    <p class="content">            
    <strong><a href="https://research.nvidia.com/labs/dir/cosmos-transfer1/">project</a></strong> |
    <strong><a href="https://arxiv.org/abs/2503.14492">paper</a></strong> |
    <strong><a href="https://github.com/nvidia-cosmos/cosmos-transfer1">code</a></strong> |
    <strong><a href="https://www.youtube.com/watch?v=0Yr5SdrVnxc">video</a></strong> |
    <strong><a shape="rect" href="javascript:togglebib('cosmos-transfer1')" class="togglebib">bibtex</a></strong> </p>
    <pre xml:space="preserve" style="display: none;">
@article{nvidia2025cosmostransfer1conditionalworldgeneration,
  title = {Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control},
  author = {NVIDIA and others},
  journal = {arXiv preprint arXiv:2503.14492},
  year = {2025},
}
    </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://research.nvidia.com/labs/dir/cosmos-predict1/images/intro.jpg" class="hoverZoomLink"><img src="https://research.nvidia.com/labs/dir/cosmos-predict1/images/intro.jpg" alt="cosmos-predict1" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Cosmos World Foundation Model Platform for Physical AI</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">NVIDIA: <strong>Ting-Chun Wang</strong> (core contributor) et al.</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>arXiv</em>, 2025
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="cosmos-predict1">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/cosmos-predict1/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2501.03575">paper</a></strong> |
        <strong><a href="https://github.com/nvidia-cosmos/cosmos-predict1">code</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=9Uch931cDx8">video</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('cosmos-predict1')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@article{nvidia2025cosmosworldfoundationmodel,
    title     = {Cosmos World Foundation Model Platform for Physical AI},
    author    = {NVIDIA and others},
    journal   = {arXiv preprint arXiv:2501.03575},
    year      = {2025},
}
        </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://research.nvidia.com/labs/dir/jedi/figure/fig_rersult.png" class="hoverZoomLink"><img src="https://research.nvidia.com/labs/dir/jedi/figure/fig_rersult.png" alt="jedi" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>JeDi: Joint-image Diffusion Models for Finetuning-free Personalized Text-to-image Generation</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><a href="https://zengxianyu.github.io">Yu Zeng</a>, <a href="https://engineering.jhu.edu/vpatel36/">Vishal M. Patel</a>, <a href="https://whc.is/">Haocheng Wang</a>, <a href="https://www.xunhuang.me/">Xun Huang</a>, <strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="https://yogeshbalaji.github.io/">Yogesh Balaji</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="jedi">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/jedi/">project</a></strong> |
        <strong><a href="https://research.nvidia.com/labs/dir/jedi/assets/joint-image-diffusion-combine.pdf">paper</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('jedi')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{zeng2024jedi,
    title={JeDi: Joint-image Diffusion Models for Finetuning-free Personalized Text-to-image Generation},
    author={Zeng, Yu and Patel, Vishal M and Wang, Haochen and Huang, Xun and Wang, Ting-Chun and Liu, Ming-Yu and Balaji, Yogesh},
    booktitle={CVPR},
    year={2024}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://grail.cs.washington.edu/projects/dreampose/static/videos/pose-demo/pose-dress1.gif" class="hoverZoomLink"><img src="https://grail.cs.washington.edu/projects/dreampose/static/videos/pose-demo/pose-dress1.gif" alt="dreampose" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://johannakarras.github.io/">Johanna Karras</a>, <a href="https://holynski.org/">Aleksander Holynski</a>, <strong>Ting-Chun Wang</strong>, <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="dreampose">    
        <p class="content">            
        <strong><a href="https://grail.cs.washington.edu/projects/dreampose/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2304.06025">paper</a></strong> |
        <strong><a href="https://github.com/johannakarras/DreamPose">code</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('dreampose')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{karras2023dreampose,
    title={{Dreampose: Fashion image-to-video synthesis via stable diffusion}},
    author={Karras, Johanna and Holynski, Aleksander and Wang, Ting-Chun and Kemelmacher-Shlizerman, Ira},
    booktitle={ICCV},
    year={2023}
}
      </pre> </div> </td> </tr>

    <tr> <td width="20%" valign="top"><a href="https://arunmallya.github.io/teasers/spacex.gif" class="hoverZoomLink"><img src="https://arunmallya.github.io/teasers/spacex.gif" alt="space" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>SPACE: Speech-driven Portrait Animation with Controllable Expression</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://research.nvidia.com/person/siddharth-gururani/">Siddharth Gururani</a>, <a href="https://arunmallya.github.io/">Arun Mallya</a>, <strong>Ting-Chun Wang</strong>, <a href="https://rafaelvalle.github.io/">Rafael Valle</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="space">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/space/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2211.09809">paper</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('space')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{gururani2022SPACE,
    title={{SPACE: Speech-driven Portrait Animation with Controllable Expression}},
    author={Siddharth Gururani and Arun Mallya and Ting-Chun Wang and Rafael Valle and Ming-Yu Liu},
    booktitle={ICCV},
    year={2023}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://arunmallya.github.io/teasers/imwa.gif" class="hoverZoomLink"><img src="https://arunmallya.github.io/teasers/imwa.gif" alt="implicit_warp" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Implicit Warping for Animation with Image Sets</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://arunmallya.github.io/">Arun Mallya</a>, <strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="implicit_warp">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/implicit_warping/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2210.01794">paper</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('implicit_warp')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{mallya2022implicit,
    title={{Implicit Warping for Animation with Image Sets}},
    author={Arun Mallya and Ting-Chun Wang and Ming-Yu Liu},
    booktitle={NeurIPS},
    year={2022}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://github.com/NVlabs/long-video-gan/raw/main/teaser_biking.gif" class="hoverZoomLink"><img src="https://github.com/NVlabs/long-video-gan/raw/main/teaser_biking.gif" alt="video_gan" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Generating Long Videos of Dynamic Scenes</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://www.timothybrooks.com/">Tim Brooks</a>, Janne Hellsten, Miika Aittala, <strong>Ting-Chun Wang</strong>, Timo Aila, Jaakko Lehtinen, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Alexei A. Efros, Tero Karras</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="video_gan">    
        <p class="content">            
        <strong><a href="https://www.timothybrooks.com/tech/long-video-gan/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2206.03429">paper</a></strong> |
        <strong><a href="https://github.com/NVlabs/long-video-gan">code</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('video_gan')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{brooks2022generating,
  title={Generating Long Videos of Dynamic Scenes},
  author={Brooks, Tim and Hellsten, Janne and Aittala, Miika and Wang, Ting-Chun and Aila, Timo and Lehtinen, Jaakko and Liu, Ming-Yu and Efros, Alexei A and Karras, Tero},
  booktitle=NeurIPS,
  year={2022}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://research.nvidia.com/labs/dir/lumos/images/teaser.jpg" class="hoverZoomLink"><img src="https://research.nvidia.com/labs/dir/lumos/images/teaser.jpg" alt="lumos" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://yuyingyeh.github.io/">Yu-Ying Yeh</a>, <a href="https://luminohope.org/">Koki Nagano</a>, <a href="https://www.samehkhamis.com/">Sameh Khamis</a>, <a href="https://jankautz.com/">Jan Kautz</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <strong>Ting-Chun Wang</strong></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong>)</em>, 2022
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="lumos">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/lumos/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2209.10510">paper</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=uWSVpG0eKbU">YouTube</a></strong> |
        <strong><a href="http://imaginaire.cc/Lumos/">demo</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('lumos')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@article{yeh2022learning,
    title={Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation},
    author={Yu-Ying Yeh and Koki Nagano and Sameh Khamis and Jan Kautz and Ming-Yu Liu and Ting-Chun Wang},
    journal={ACM Transactions on Graphics (TOG)},
    year={2022}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://research.nvidia.com/labs/dir/PoE-GAN/images/samples/panorama/reflection.jpg" class="hoverZoomLink"><img src="https://research.nvidia.com/labs/dir/PoE-GAN/images/samples/panorama/reflection.jpg" alt="poegan" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Multimodal Conditional Image Synthesis with Product-of-Experts GANs</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://xunhuang.me/">Xun Huang</a>, <a href="https://arunmallya.github.io/">Arun Mallya</a>, <strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="poegan">    
        <p class="content">            
        <strong><a href="https://research.nvidia.com/labs/dir/PoE-GAN/">project</a></strong> |
        <strong><a href="https://arxiv.org/abs/2112.05130">paper</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=56aA_FaeAPY">YouTube</a></strong> |
        <strong><a href="http://imaginaire.cc/gaugan2/">demo</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('poegan')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{huang2022poegan,
    title={Multimodal Conditional Image Synthesis with Product-of-Experts {GANs}},
    author={Xun Huang and Arun Mallya and Ting-Chun Wang and Ming-Yu Liu},
    booktitle={ECCV},
    year={2022}
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="papers/PAMI22/teaser.jpg" class="hoverZoomLink"><img src="papers/PAMI22/teaser.jpg" alt="pconv_tpami" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Partial Convolution for Padding, Inpainting, and Image Synthesis</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Aysegul Dundar, Kevin J. Shih, <strong>Ting-Chun Wang</strong>, Fitsum A. Reda, <a href="https://karansapra.github.io/">Karan Sapra</a>, <a href="https://chrisding.github.io/">Zhiding Yu</a>, Xiaodong Yang, Andrew Tao, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2022
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="pconv_tpami">    
        <p class="content">            
        <strong><a href="https://ieeexplore.ieee.org/document/9903574">paper</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('pconv_tpami')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@article{liu2022partial,
  title={Partial Convolution for Padding, Inpainting, and Image Synthesis}, 
  author={Liu, Guilin and Dundar, Aysegul and Shih, Kevin J. and Wang, Ting-Chun and Reda, Fitsum A. and Sapra, Karan and Yu, Zhiding and Yang, Xiaodong and Tao, Andrew and Catanzaro, Bryan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  year={2022},
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://nvlabs.github.io/face-vid2vid/web_gifs/teaser.gif" class="hoverZoomLink"><img src="https://nvlabs.github.io/face-vid2vid/web_gifs/teaser_novelview.gif" alt="facevid2vid" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="https://arunmallya.github.io/">Arun Mallya</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
          <font color="red">(oral presentation)</font>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="facevid2vid">    
        <p class="content">            
        <strong><a href="https://nvlabs.github.io/face-vid2vid">project</a></strong> |
        <strong><a href="https://nvlabs.github.io/face-vid2vid/main.pdf">paper</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=nLYg9Waw72U">YouTube</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=smrcnZ5Eg4A">Talk</a></strong> |
        <strong><a href="https://drive.google.com/open?id=1fsp5c3lb56JC-2vKiD2O3EYENnc9iorU">Slides</a></strong> |
        <strong><a href="https://drive.google.com/file/d/14Pou1qnmGqHrGZMq6RjclNA9P6s7Lhub">Poster</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('facevid2vid')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2021facevid2vid,
   author    = {Ting-Chun Wang and Arun Mallya and Ming-Yu Liu},
   title     = {One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing},
   booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
   year      = {2021},
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="https://liuguilin1225.github.io/img/front_page.PNG" class="hoverZoomLink"><img src="https://liuguilin1225.github.io/img/front_page.PNG" alt="transposer_tog" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Rohan Taori, <strong>Ting-Chun Wang</strong>, <a href="https://chrisding.github.io/">Zhiding Yu</a>, Shiqiu Liu, Fitsum A. Reda, <a href="https://karansapra.github.io/">Karan Sapra</a>, Andrew Tao, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>ACM Transactions on Graphics</em>, 2021 (provisionally accepted)</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="transposer_tog">    
        <p class="content">    
        <strong><a href="https://arxiv.org/pdf/2007.07243.pdf">paper</a></strong> |        
        <strong><a href="https://www.youtube.com/watch?v=ej1NDiMT99g&ab_channel=GuilinLiu">YouTube</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('transposer_tog')" class="togglebib">bibtex</a></strong></p>        
        <pre xml:space="preserve" style="display: none;">
@article{liu2021transposer,
   author  = {Guilin Liu and Rohan Taori and Ting-Chun Wang and Zhiding Yu and Shiqiu Liu 
              and Fitsum Reda and Karan Sapra and Andrew Tao and Bryan Catanzaro},
   title   = {Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter},
   journal = {ACM Transactions on Graphics},
   year      = {2021}
}
      </pre> </div> </td> </tr> 

  <tr> <td width="20%" valign="top"><a href="papers/arxiv/survey_teaser.png" class="hoverZoomLink"><img src="papers/arxiv/survey_teaser.png" alt="gan_survey" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cornell.edu/~xhuang/">Xun Huang</a>, <a href="https://jiahuiyu.com/">Jiahui Yu</a>, <strong>Ting-Chun Wang</strong>, <a href="https://arunmallya.github.io/">Arun Mallya</a> </p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Proceedings of The IEEE</em>, 2021
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="gan_survey">    
        <p class="content">                    
        <strong><a href="https://arxiv.org/pdf/2008.02793.pdf">paper</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('gan_survey')" class="togglebib">bibtex</a></strong> </p>
        <pre xml:space="preserve" style="display: none;">          
@article{liu2021generative,
   title={Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications},
   author={Ming-Yu Liu and Xun Huang and Jiahui Yu and Ting-Chun Wang and Arun Mallya},
   journal={Proceedings of The IEEE},
   year={2021}
}
  </pre> </div> </td> </tr>  

  <tr> <td width="20%" valign="top"><a href="https://nvlabs.github.io/wc-vid2vid/images/guidance.png" class="hoverZoomLink"><img src="https://nvlabs.github.io/wc-vid2vid/images/guidance.png" alt="wcvid2vid_eccv" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>World-Consistent Video-to-Video Synthesis</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://arunmallya.github.io/">Arun Mallya</a>*, <strong>Ting-Chun Wang</strong>*, <a href="https://karansapra.github.io/">Karan Sapra</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a> (*equal contribution)</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020        
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="wcvid2vid_eccv">    
        <p class="content">            
        <strong><a href="https://nvlabs.github.io/wc-vid2vid/">project</a></strong> |        
        <strong><a href="https://nvlabs.github.io/wc-vid2vid/files/wc-vid2vid.pdf">paper</a></strong> |         
        <strong><a href="https://www.youtube.com/watch?v=rlCh6-2NfSg">YouTube</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('wcvid2vid_eccv')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">          
@inproceedings{mallya2020world,
   author    = {Arun Mallya and Ting-Chun Wang and Karan Sapra and Ming-Yu Liu},
   title     = {World-Consistent Video-to-Video Synthesis},  
   booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},   
   year      = {2020},
}
      </pre> </div> </td> </tr>

  <tr> <td width="20%" valign="top"><a href="papers/PAMI20/teaser.PNG" class="hoverZoomLink"><img src="papers/PAMI20/teaser.PNG" alt="ds_tpami" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Domain Stylization: A Fast Covariance Matching Framework towards Domain Adaptation</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Aysegul Dundar, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="https://chrisding.github.io/">Zhiding Yu</a>, <strong>Ting-Chun Wang</strong>, John Zedlewski, <a href="http://jankautz.com/">Jan Kautz</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2020</p>
        <p class="margin">&nbsp;</p>
        <p></p>
        <div class="paper" id="ds_tpami">         
        <p class="content">               
        <strong><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8968319">paper</a></strong> |         
        <strong><a shape="rect" href="javascript:togglebib('ds_tpami')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@article{dundar2020domain,
  title={Domain Stylization: A Fast Covariance Matching Framework towards Domain Adaptation},
  author={Aysegul Dundar and Ming-Yu Liu and Zhiding Yu and Ting-Chun Wang and John Zedlewski and and Jan Kautz},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year={2020}
}
      </pre></div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://nvlabs.github.io/few-shot-vid2vid/web_gifs/dance.gif" class="hoverZoomLink"><img src="https://nvlabs.github.io/few-shot-vid2vid/web_gifs/dance.gif" alt="avid2vid_nips" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Few-shot Video-to-Video Synthesis</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"> <strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Andrew Tao, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2019</p>
        <p class="margin">&nbsp;</p>
        <p></p>
        <div class="paper" id="avid2vid_nips">         
        <p class="content">       
        <strong><a href="https://nvlabs.github.io/few-shot-vid2vid">project</a></strong> |        
        <strong><a href="https://arxiv.org/abs/1910.12713">paper</a></strong> | 
        <strong><a href="https://github.com/NVLabs/few-shot-vid2vid">github</a></strong> |
        <strong><a href="https://youtu.be/8AZBuyEuDqc">YouTube</a></strong> |                        
        <strong><a shape="rect" href="javascript:togglebib('avid2vid_nips')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2019fewshotvid2vid,
  title={Few-shot Video-to-Video Synthesis},
  author={Ting-Chun Wang and Ming-Yu Liu and Andrew Tao and Guilin Liu and Jan Kautz and Bryan Catanzaro},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
      </pre></div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://raw.githubusercontent.com/NVlabs/Dancing2Music/master/imgs/example.gif" class="hoverZoomLink"><img src="https://raw.githubusercontent.com/NVlabs/Dancing2Music/master/imgs/example.gif" alt="d2m_nips" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Dancing to Music</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"> <a href="http://vllab.ucmerced.edu/hylee/">Hsin-Ying Lee</a>, <a href="https://xiaodongyang.org/">Xiaodong Yang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <strong>Ting-Chun Wang</strong>, <a href="https://jonlu0602.github.io/">Yu-Ding Lu</a>, <a href="https://www.ucmerced.edu/content/ming-hsuan-yang">Ming-Hsuan Yang</a>, <a href="http://jankautz.com/">Jan Kautz</a>          
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2019</p>
        <p class="margin">&nbsp;</p>
        <p></p>
        <div class="paper" id="dance2music_nips">         
        <p class="content">                 
        <strong><a href="https://arxiv.org/abs/1911.02001">paper</a></strong> | 
        <strong><a href="https://github.com/NVlabs/Dancing2Music">github</a></strong> |
        <strong><a href="https://youtu.be/-e9USqfwZ4A">YouTube</a></strong> |                        
        <strong><a shape="rect" href="javascript:togglebib('dance2music_nips')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@inproceedings{lee2019dancing2music,
  title={Dancing to Music},
  author={Hsin-Ying Lee and Xiaodong Yang and Ming-Yu Liu and Ting-Chun Wang and Yu-Ding Lu and Ming-Hsuan Yang and Jan Kautz},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2019}
}
      </pre></div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://nvlabs.github.io/SPADE/images/ocean.gif" class="hoverZoomLink"><img src="https://nvlabs.github.io/SPADE/images/ocean.gif" alt="spade_cvpr" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Semantic Image Synthesis with Spatially-Adaptive Normalization</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"> <a href="https://taesung.me/">Taesung Park</a>,  <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <strong>Ting-Chun Wang</strong>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
        <font color="red">(best paper finalist)</font></p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="spade_cvpr">    
        <p class="content">    
        <strong><a href="https://nvlabs.github.io/SPADE">project</a></strong> |        
        <strong><a href="https://arxiv.org/abs/1903.07291">paper</a></strong> | 
        <strong><a href="https://github.com/NVLabs/SPADE">github</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=MXWm6w4E5q0">YouTube</a></strong> |                        
        <strong><a shape="rect" href="javascript:togglebib('spade_cvpr')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@inproceedings{park2019SPADE,
  title={Semantic Image Synthesis with Spatially-Adaptive Normalization},
  author={Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE Conference on 
             Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://raw.github.com/NVIDIA/vid2vid/master/imgs/teaser.gif" class="hoverZoomLink"><img src="https://github.com/NVIDIA/vid2vid/raw/master/imgs/teaser.gif" alt="vid2vid_nips" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Video-to-Video Synthesis</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, 
          Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2018</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="vid2vid_nips">    
        <p class="content">    
        <strong><a href="https://tcwang0509.github.io/vid2vid/">project</a></strong> |
        <strong><a href="https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf">paper</a></strong> |
        <strong><a href="https://arxiv.org/pdf/1808.06601.pdf">arXiv</a></strong> | 
        <strong><a href="https://github.com/NVIDIA/vid2vid">github</a></strong> |
        <strong><a href="https://goo.gl/KmFLBS">YouTube</a></strong> |                        
        <strong><a shape="rect" href="javascript:togglebib('vid2vid_nips')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top" align="center"><a href="papers/arxiv/partialconv_teaser.PNG" class="hoverZoomLink"><img src="papers/arxiv/partialconv_teaser.PNG" alt="partconv_arxiv" width="60%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Partial Convolution based Padding</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Kevin J. Shih, <strong>Ting-Chun Wang</strong>, Fitsum A. Reda, <a href="https://karansapra.github.io/">Karan Sapra</a>, <a href="https://chrisding.github.io/">Zhiding Yu</a>, Andrew Tao, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content">arXiv preprint arXiv:1811.11718</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="partconv_arxiv">    
        <p class="content">                    
        <strong><a href="https://arxiv.org/abs/1811.11718">arXiv</a></strong> |                 
        <strong><a shape="rect" href="javascript:togglebib('partconv_arxiv')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">          
@article{liu2018partial,
   author    = {Liu, Guilin and Shih, Kevin J and Wang, Ting-Chun and Reda, Fitsum A 
                and Sapra, Karan and Yu, Zhiding and Tao, Andrew and Catanzaro, Bryan},
   title     = {Partial convolution based padding},
   journal   = {arXiv preprint arXiv:1811.11718},
   year      = {2018},
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="papers/arxiv/ds_teaser.jpg" class="hoverZoomLink"><img src="papers/arxiv/ds_teaser.jpg" alt="ds_arxiv" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Aysegul Dundar, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <strong>Ting-Chun Wang</strong>, John Zedlewski, <a href="http://jankautz.com/">Jan Kautz</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content">arXiv preprint arXiv:1807.09384</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="ds_arxiv">    
        <p class="content">                    
        <strong><a href="https://arxiv.org/abs/1807.09384">arXiv</a></strong> |           
        <strong><a shape="rect" href="javascript:togglebib('ds_arxiv')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@article{dundar2018domain,
  title={Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation},
  author={Dundar, Aysegul and Liu, Ming-Yu and Wang, Ting-Chun and Zedlewski, John and Kautz, Jan},
  journal={arXiv preprint arXiv:1807.09384},
  year={2018}
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="papers/ECCV18/teaser.jpg" class="hoverZoomLink"><img src="papers/ECCV18/teaser.jpg" alt="parconv_eccv" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Image Inpainting for Irregular Holes Using Partial Convolutions</heading></a><br>
        <p class="margin">&nbsp;</p>        
        <p class="content"><a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Fitsum A. Reda, Kevin J. Shih, <strong>Ting-Chun Wang</strong>, Andrew Tao, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2018        
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="partconv_eccv">    
        <p class="content">            
        <strong><a href="https://nv-adlr.github.io/publication/partialconv-inpainting">project</a></strong> |        
        <strong><a href="https://arxiv.org/pdf/1804.07723.pdf">arXiv</a></strong> |         
        <strong><a href="https://www.youtube.com/watch?v=gg0F5JjKmhA">YouTube</a></strong> |                        
        <strong><a shape="rect" href="javascript:togglebib('partconv_eccv')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">          
@inproceedings{liu2018image,
   author    = {Liu, Guilin and Reda, Fitsum A and Shih, Kevin J 
                and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
   title     = {Image inpainting for irregular holes using partial convolutions},
   booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},   
   year      = {2018},
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://raw.github.com/NVIDIA/pix2pixHD/master/imgs/teaser_style.gif" class="hoverZoomLink"><img src="https://github.com/NVIDIA/pix2pixHD/raw/master/imgs/teaser_720.gif" alt="pix2pixHD_cvpr" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, 
        <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
        <font color="red">(oral presentation)</font></p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="pix2pixHD_cvpr">    
        <p class="content">    
        <strong><a href="https://tcwang0509.github.io/pix2pixHD/">project</a></strong> |        
        <strong><a href="papers/CVPR18/pix2pixHD.pdf">paper</a></strong> |
        <strong><a href="https://arxiv.org/pdf/1711.11585.pdf">arXiv</a></strong> |         
        <strong><a href="https://github.com/NVIDIA/pix2pixHD">github</a></strong> |
        <strong><a href="https://goo.gl/YWuH8r">YouTube</a></strong> |              
        <strong><a href="papers/CVPR18/pix2pixHD_slides.pptx">slides</a></strong> |                  
        <strong><a shape="rect" href="javascript:togglebib('pix2pixHD_cvpr')" class="togglebib">bibtex</a></strong> </p>        
        <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2018pix2pixHD,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu 
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {High-Resolution Image Synthesis and Semantic Manipulation 
                with Conditional GANs},
   booktitle = {Proceedings of the IEEE Conference on 
                Computer Vision and Pattern Recognition (CVPR)},   
   year      = {2018},
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-50.pdf" class="hoverZoomLink"><img src="files/thesis.jpg" alt="thesis" width="100%"></a>
        <td width="80%" valign="top"><p>
        <heading>Beyond Photo-Consistency: Shape, Reflectance, and Material Estimation Using<br> Light-Field Cameras</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Ting-Chun Wang</p>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-50.pdf">PhD Thesis</a></strong>, 2017</p>
        <p class="margin">&nbsp;</p>
      </td> </tr>

      <!-- <tr> <td width="20%" valign="top"><a href="papers/SIG17/lfv/teaser_highres.jpg" class="hoverZoomLink"><img src="papers/SIG17/lfv/teaser.jpg" alt="lfv_sig" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Light Field Video Capture Using a Learning-Based Hybrid Imaging System</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://nkhademi.com/">Nima Khademi Kalantari</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>ACM Transactions on Graphics (<strong>SIGGRAPH</strong>)</em>, 2017</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="lfv_sig">    
        <p class="content">    
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video.pdf">paper</a></strong> |
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video_LoRes.pdf">low-res pdf</a></strong> |              
        <strong><a href="https://www.youtube.com/watch?v=TqVKcssYfAo&feature=youtu.be">YouTube</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('lfv_sig')" class="togglebib">bibtex</a></strong> |        
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/">project page</a></strong></p>        
        <pre xml:space="preserve" style="display: none;">
@article{wang2017light,
   author  = {Ting-Chun Wang and Jun-Yan Zhu and Nima Khademi Kalantari 
              and Alexei A. Efros and Ravi Ramamoorthi},
   title   = {Light Field Video Capture Using a Learning-Based Hybrid 
              Imaging System},
   journal = {ACM Transactions on Graphics (Proceedings of SIGGRAPH)},
   volume  = {36},
   number  = {4},
   year    = {2017},
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="papers/PAMI17/model_highres.png" class="hoverZoomLink"><img src="papers/PAMI17/model.png" alt="brdf_pami" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p><heading>SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</heading><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2017</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="brdf_pami">
        <p class="content">
        <strong><a href="papers/PAMI17/BRDF_TPAMI.pdf">paper</a></strong> |        
        <strong><a shape="rect" href="javascript:togglebib('brdf_pami')" class="togglebib">bibtex</a></p></strong>        
        <pre xml:space="preserve" style="display: none;">
@article{wang2017svbrdf,
   title={{SVBRDF}-Invariant Shape and Reflectance 
   Estimation from Light-Field Cameras},
   author={Wang, Ting-Chun and Chandraker, Manmohan
   and Efros, Alexei and Ramamoorthi, Ravi},
   journal={IEEE Transactions on Pattern 
   Analysis and Machine Intelligence (TPAMI)},
   year={2017},
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/index_files/teaserSmall.png" class="hoverZoomLink"><img src="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/index_files/teaserSmall.png" alt="lfvs_sig" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Learning-Based View Synthesis for Light Field Cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://nkhademi.com/">Nima Khademi Kalantari</a>, <strong>Ting-Chun Wang</strong>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>      
              
      <div class="paper" id="lfvs_sig">
      <p class="content">
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/">project page</a> </strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/PaperData/SIGGRAPHAsia16_ViewSynthesis.pdf">paper</a></strong> |          
      <strong><a shape="rect" href="javascript:togglebib('lfvs_sig')" class="togglebib">bibtex</a></strong> </p>
      <pre xml:space="preserve" style="display: none;">
@article{Kalantari16ViewSynthesis,
   author={Nima Khademi Kalantari and Ting-Chun Wang 
   and Ravi Ramamoorthi},
   title={Learning-Based View Synthesis for Light 
   Field Cameras},
   journal={ACM Transactions on Graphics (Proceedings 
   of SIGGRAPH Asia 2016)},
   year={2016},
}
      </pre> </div> </td> </tr>
      
      <tr> <td width="20%" valign="top"><a href="papers/ECCV16/lfmr_highres.png" class="hoverZoomLink"><img src="papers/ECCV16/lfmr.png" alt="lfmr_eccv" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>A 4D Light-Field Dataset and CNN Architectures for Material Recognition</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><b>Ting-Chun Wang</b>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, Ebi Hiroaki, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>      
              
      <div class="paper" id="lfmr_eccv">
      <p class="content">
      <strong><a href="papers/ECCV16/LFMR.pdf">paper</a></strong> |      
      <strong><a href="papers/ECCV16/full_scene.html">HTML comparison</a></strong> |       
      <strong><a shape="rect" href="javascript:togglebib('lfmr_eccv')" class="togglebib">bibtex</a></strong> |
      <strong><a href="papers/ECCV16/dataset.html">dataset (2D thumbnail)</a></strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV16/LF_dataset.zip">full dataset (15.9G)</a> </p> </strong>                    
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016dataset,
   title={A {4D} light-field dataset and {CNN} 
   architectures for material recognition},
   author={Wang, Ting-Chun and Zhu, Jun-Yan 
   and Hiroaki, Ebi and Chandraker, Manmohan 
   and Efros, Alexei and Ramamoorthi, Ravi},
   booktitle={Proceedings of European Conference on 
   Computer Vision (ECCV)},
   year={2016}
}
      </pre> </div> </td> </tr>
      
      <tr> <td width="20%" valign="top"><a href="papers/CVPR16/brdf_highres.png" class="hoverZoomLink"><img src="papers/CVPR16/brdf.png" alt="brdf_cvpr" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</a><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a><br></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2016
      <font color="red">(oral presentation)</font></p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="brdf_cvpr">
      <p class="content">
      <strong><a href="papers/CVPR16/LF_BRDF.pdf">paper</a></strong> |      
      <strong><a href="papers/CVPR16/supplementary_document.pdf">supplementary</a></strong> |   
      <strong><a href="papers/CVPR16/MERL.html">HTML comparison</a></strong> |                 
      <strong><a shape="rect" href="javascript:togglebib('brdf_cvpr')" class="togglebib">bibtex</a></p></strong>                    
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016svbrdf,
   title={SVBRDF-invariant shape and reflectance 
   estimation from light-field cameras},
   author={Wang, Ting-Chun and Chandraker, Manmohan 
   and Efros, Alexei and Ramamoorthi, Ravi},
   booktitle={Proceedings of the IEEE Conference on 
   Computer Vision and Pattern Recognition (CVPR)},
   year={2016}
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="papers/CVPR16/stereo_highres.png" class="hoverZoomLink"><img src="papers/CVPR16/stereo.png" alt="stereo_cvpr" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth from Semi-Calibrated Stereo and Defocus</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.msrik.com/">Manohar Srikanth</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="stereo_cvpr">
      <p class="content">
      <strong><a href="papers/CVPR16/semi_stereo.pdf">paper</a></strong> |      
      <strong><a href="papers/CVPR16/index.html">HTML comparison</a></strong> |                      
      <strong><a shape="rect" href="javascript:togglebib('stereo_cvpr')" class="togglebib">bibtex</a></p></strong>                    
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016semi,
   title={Depth from semi-calibrated stereo and defocus},
   author={Wang, Ting-Chun and Srikanth, Manohar
   and Ramamoorthi, Ravi},
   booktitle={Proceedings of the IEEE Conference on 
   Computer Vision and Pattern Recognition (CVPR)},
   year={2016}
}
      </pre> </div> </td> </tr> 
      
    <tr> <td width="20%" valign="top"><a href="papers/PAMI16/model_highres.png" class="hoverZoomLink"><img src="papers/PAMI16/model.png" alt="occlusion_pami" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth Estimation with Occlusion Modeling Using Light-field Cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2016 </p>      
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="occlusion_pami">
      <p class="content">
      <strong><a href="papers/PAMI16/occlusion_TPAMI.pdf">paper</a></strong> |      
      <strong><a shape="rect" href="javascript:togglebib('occlusion_pami')" class="togglebib">bibtex</a></p></strong>      
      <pre xml:space="preserve" style="display: none;">
@article{wang2016depth,
   title={Depth estimation with occlusion modeling 
   using light-field cameras},
   author={Wang, Ting-Chun and Efros, Alexei and 
   Ramamoorthi, Ravi},
   journal={IEEE Transactions on Pattern 
   Analysis and Machine Intelligence (TPAMI)},
   volume={38},
   number={11},
   pages={2170--2181},
   year={2016},
}
      </pre> </div> </td> </tr> 
      
      <tr>
      <td width="20%" valign="top"><a href="papers/ICCV15/teaser_highres.jpg" class="hoverZoomLink"><img src="papers/ICCV15/teaser.jpg" alt="occlusion" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Occlusion-aware depth estimation using light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2015 </p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="occlusion">
      <p class="content">
      <strong><a href="papers/ICCV15/LF_occlusion_ICCV15.pdf">paper</a></strong> |      
      <strong><a href="papers/ICCV15/supplementary.html">supplementary</a></strong> | 
      <strong><a shape="rect" href="javascript:togglebib('occlusion')" class="togglebib">bibtex</a></strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ICCV15/occCode.zip">code</a></strong> | 
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ICCV15/dataset.zip">dataset (3.3GB)</a></p></strong>      
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2015occlusion,
  title={Occlusion-aware depth estimation 
  using light-field cameras.},
  author={Wang, Ting-Chun and 
  Efros, Alexei and Ramamoorthi, Ravi},
  booktitle={Proceedings of the IEEE International 
  Conference on Computer Vision (ICCV)},
  year={2015}
}
      </pre> </div> </td> </tr> 
            
      
      <tr>
      <td width="20%" valign="top"><a href="papers/PAMI15/specular_highres.png" class="hoverZoomLink"><img src="papers/PAMI15/specular.png" alt="glossy_pami" width="100%"  height="150" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth estimation and specular removal for glossy surfaces using <br>
          point and line consistency with light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://www.cs.berkeley.edu/~mtao/">Michael Tao</a>, Jong-Chyi Su, <strong>Ting-Chun Wang</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em></p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="glossy_pami">
      <p class="content">
      <strong><a href="papers/PAMI15/LF_glossy_TPAMI.pdf">paper</a></strong> |      
      <strong><a shape="rect" href="javascript:togglebib('glossy_pami')" class="togglebib">bibtex</a> </p></strong>      
      <pre xml:space="preserve" style="display: none;">
@article{tao2015depth,
title={Depth Estimation and Specular Removal 
for Glossy Surfaces Using Point and 
Line Consistency with Light-Field Cameras},
author={Tao, Michael and Su, Jong-Chyi 
and Wang, Ting-Chun and Malik, Jitendra 
and Ramamoorthi, },
journal={IEEE Transactions on Pattern 
Analysis \& Machine Intelligence},
number={1},
pages={1--1},
year={2015},
publisher={IEEE}
}
      </pre></div></td></tr>
      
      
      <tr>
      <td width="20%" valign="top"><a href="papers/ECCV14/l4cv_highres.png" class="hoverZoomLink"><img src="papers/ECCV14/l4cv.jpg" alt="glossy" width="100%" height="150" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth estimation for glossy surfaces with light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://www.cs.berkeley.edu/~mtao/">Michael Tao</a>, <strong>Ting-Chun Wang</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>ECCV workshop on Light Fields for Computer Vision (<strong>L4CV</strong>)</em>, 2014</p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="glossy">
      <p class="content">
      <strong><a href="papers/ECCV14/LF_glossy_ECCV14_s.pdf">paper</a></strong> |      
      <strong><a shape="rect" href="javascript:togglebib('glossy')" class="togglebib">bibtex</a> </p> </strong>      
      <pre xml:space="preserve" style="display: none;">
@inproceedings{tao2014depth,
title={Depth estimation for glossy 
surfaces with light-field cameras},
author={Tao, Michael W and Wang, Ting-Chun 
and Malik, Jitendra and Ramamoorthi, Ravi},
booktitle={Computer Vision-ECCV 2014 Workshops},
pages={533--547},
year={2014},
organization={Springer}
}
      </pre> </div> </td> </tr> -->
      </table>
  </div>
  </div>
    </div>
    <div id="footer">
      
</div>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>


<br>
<p class="title-large">Professional Activities</p>
<p class="margin">&nbsp;</p>

<p class="content"><b>Tutorial/Workshop Co-organizer</b></p>
<ul>
<li><p class="content">CVPR 2023: <strong><a href="https://ai4cc.net/">AI for Content Creation Workshop</a></strong></p></li>
<li><p class="content">CVPR 2022: <strong><a href="https://ai4cc.net/2022/">AI for Content Creation Workshop</a></strong></p></li>
<li><p class="content">ECCV 2020: <strong><a href="https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/">Tutorial on Accelerating Computer Vision with Mixed Precision</a></strong></p></li>
<li><p class="content">ICCV 2019: <strong><a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/">Tutorial on Accelerating Computer Vision with Mixed Precision</a></strong></p></li>
<li><p class="content">ICIP 2019: <strong><a href="https://mingyuliutw.github.io/icip2019-image-translation-tutorial/">Tutorial on Image-to-Image Translation</a></strong></p></li>
</ul>

<p class="content"><b>Conference Area Chair</b>: CVPR, ECCV, ICLR, WACV</p>
<p class="margin">&nbsp;</p>
<p class="content"><b>Conference Reviewer</b>: CVPR, ICCV, ECCV, SIGGRAPH, SIGGRAPH Asia, Eurographics, NeurIPS, ICLR, ICRA</p>
<p class="margin">&nbsp;</p>
<p class="content"><b>Journal Reviewer</b>: TPAMI, IJCV, TOG, TIP, TMM, TVCG, TCSVT, CVIU</p>
<br>


<br>
<p class="title-large">Awards</p>
<ul>
<li><p class="content">Best in Show Award and Audience Choice Award, RealTimeLive, SIGGRAPH 2019</p></li>
<li><p class="content">Best Paper Finalist, CVPR 2019</p></li>
<li><p class="content">1st place, Domain Adaptation for Semantic Segmentation Competition, WAD Challenge, CVPR 2018</p></li>
<li><p class="content">NTECH Best Paper Award, NVIDIA 2018</p></li>
<li><p class="content">Pioneer Research Award, NVIDIA 2018 </p></li>
</ul>


<br>
<p class="title-large">Software</p>
<p class="content"><strong><a href="https://github.com/NVlabs/imaginaire">Imaginaire</a></strong>: a common codebase for training generative models.</p>
<p class="content"><strong><a href="https://github.com/NVLabs/few-shot-vid2vid">Few-shot vid2vid</a></strong>: Few-shot video-to-video translation.</p>
<p class="content"><strong><a href="https://github.com/NVLabs/SPADE">SPADE</a></strong>: Semantic image synthesis on diverse datasets including Flickr and coco.</p>
<p class="content"><strong><a href="https://github.com/NVIDIA/vid2vid">vid2vid</a></strong>: High-resolution video-to-video translation.</p>
<p class="content"><strong><a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a></strong>: High-resolution image-to-image translation.</p>
<!-- <p class="content"><strong><a href="https://github.com/junyanz/light-field-video">Light Field Video</a></strong>: Light field video applications (e.g. video refocusing, changing aperture and view).</p> -->
<br>


<br>
<p class="title-large">Talks</p>
<p class="content"><strong><a href="https://www.youtube.com/watch?v=smrcnZ5Eg4A">Face-vid2vid: One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing</a></strong></p>
<p class="content">CVPR (2021)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=1ZBgas7OvW8ZV-40hV4v284ngDFd8XvgE">Few-Shot Video-to-Video Synthesis</a></strong></p>
<p class="content">ICCV Workshop on Advances in Image Manipulation (2019)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/tingchunw_nvidia_mixed_precision_for_pix2pixHD.pdf">Mixed-Precision Training for pix2pixHD</a></strong></p>
<p class="content">ICCV Tutorial on Accelerating Computer Vision with Mixed Precision (2019)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=1qCoFdr3HJtLUFDKDpjomzhrXu3iJjcEz">Image-to-Image Translation</a></strong></p>
<p class="content">ICIP Tutorial on Image-to-Image Translation (2019)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://nvlabs.github.io/dl-for-content-creation/"> Video-to-Video Synthesis </a></strong></p>
<p class="content">CVPR Workshop on Deep Learning for Content Creation (2019)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://youtu.be/sIkUzmgUaxc?t=1200" target="_blank" rel="nofollow" class="caption-2">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (pix2pixHD)</a></strong></p>
<p class="content">CVPR (2018)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaVkpxcVpIOGhfeTQ">Beyond Photo-Consistency: Shape, Reflectance, and Material Estimation Using Light-Field Cameras</a></strong></p>
<p class="content">Dissertation talk (2017)</p>
<p class="margin">&nbsp;</p>

<!-- <p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaQU9DeXJDRG81ZXc">Light Field Video Capture Using a Learning-Based Hybrid Imaging System</a></strong></p>
<p class="content">Siggraph (2017)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaMHpUVkNpM2JWbms">SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</a></strong></p>
<p class="content">CVPR (2016)</p> -->

<br>


<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="./files/counter.js.download"></script><script language="javascript">var _go_js="1.0";</script><script language="javascript1.1">_go_js="1.1";</script><script language="javascript1.2">_go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script><script language="javascript1.4">_go_js="1.4";</script><script language="javascript1.5">_go_js="1.5";</script><script language="javascript1.6">_go_js="1.6";</script><script language="javascript1.7">_go_js="1.7";</script><script language="javascript1.8">_go_js="1.8";</script><script language="javascript1.9">_go_js="1.9";</script><script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
<!-- <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> -->
</div>
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body></html>