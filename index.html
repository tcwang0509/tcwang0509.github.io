
<!-- saved from url=(0041)http://people.eecs.berkeley.edu/~junyanz/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<script async src="./analytics.js"></script><script type="text/javascript" src="./hidebib.js"></script>
<title>Ting-Chun Wang's Homepage</title>
<link rel="shortcut icon" href="http://cg.cs.tsinghua.edu.cn/people/~xianying/favicon.ico">
<style type="text/css">
body {
	margin-top: 30px;
	margin-bottom: 30px;
	margin-left: 100px;
	margin-right: 100px;
}
p {
	margin-top: 0px;
	margin-bottom: 0px;
}

.caption {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #990000;
}
.caption-3 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #F00;
}
.caption-4 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.content {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
}
.content a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #000;
}
.content strong a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
heading {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
  font-size: 15px;
  font-weight: 700
}
.title-small {
	font-size: 20px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #F90;
}
.title-large {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #000;
}
.margin {
	font-size: 10px;
	line-height: 10px;
}
.margin-small {
	font-size: 5px;
	line-height: 5px;
}
.margin-large {
	font-size: 16px;
	line-height: 16px;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
content a:link {
	text-decoration: none;
}
content a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: underline;
	color: #06F;
	font-family: Tahoma, Geneva, sans-serif;
}
strong a:active {
	text-decoration: underline;
	color: #06F;
}
</style>
<script async="" src="./files/analytics.js.download"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script></head>



<body>

<table border="0" width="100%">
  <tbody>

    <tr>
    <td width="185"><img src="./files/portrait.jpg" border="1" height="250"></td>
    <td width="15"></td>
    <td></td>
    <td><table border="0" width="100%">
      <tbody><tr height="10">
        <td colspan="2"></td></tr>


         <tr height="20">
        <td>
           <p class="caption">Ting-Chun Wang</p>
           <p class="content">Research Scientist</p>
           <p class="content">NVIDIA</p>           
           <p class="content">Santa Clara, CA</p>
        </td>
      </tr>

      <tr height="40">
        <td><table border="0" width="100%">
          <tbody><tr height="20">
            <td width="55">
              <p class="content"><strong>Email: </strong></p></td>
            <td>
              <p class="content"> tingchunw at nvidia dot com</p></td>
          </tr>          
        </tbody></table></td>
      </tr>

      <tr height="20">
        <td>
          <p class="margin">&nbsp;</p>
          <p class="content"><strong><a href="files/CV_Ting-Chun_Wang.pdf">CV</a></strong> | 
          <strong><a href="https://github.com/tcwang0509/">GitHub</a></strong> | 
          <strong><a href="https://scholar.google.com/citations?user=ajXAb54AAAAJ&hl=en">Google Scholar</a></strong> </p>
        </td>
      </tr>
      <tr height="20">
        <td colspan="2"></td></tr>
    </tbody></table></td>
  </tr>
</tbody></table>
<p class="margin">&nbsp;</p>

<table border="0">
  <tbody>
    <tr>
      <td width="1000"> <p align="justify" class="content">I'm a research scientist at NVIDIA, working on computer vision, machine learning and computer graphics. <br>I received my PhD from University of California, Berkeley in 2017, advised by Professor <strong><a href="https://cseweb.ucsd.edu/~ravir/" target="_blank" rel="nofollow" class="caption-2">Ravi Ramamoorthi</a></strong> and <strong><a href="http://www.eecs.berkeley.edu/~efros/" target="_blank" rel="nofollow" class="caption-2">Alexei A. Efros</a></strong>. </p>
        <br>
    </td></tr>
  </tbody>
</table>

<br>


<p id="sect-publications" class="title-large">Publications</p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">   

      <tr> <td width="20%" valign="top"><a href="https://raw.github.com/NVIDIA/pix2pixHD/master/imgs/teaser_style.gif" class="hoverZoomLink"><img src="https://github.com/NVIDIA/pix2pixHD/raw/master/imgs/teaser_720.gif" alt="pix2pixHD_cvpr" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, 
        <a href="http://catanzaro.name/">Bryan Catanzaro</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
        <font color="red">(oral presentation)</font></p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="pix2pixHD_cvpr">    
        <p class="content">    
        <strong><a href="https://tcwang0509.github.io/pix2pixHD/">project</a></strong> |
        <strong><a href="papers/CVPR18/pix2pixHD.pdf">paper</a></strong> |
        <strong><a href="https://arxiv.org/pdf/1711.11585.pdf">arXiv</a></strong> | 
        <strong><a href="papers/CVPR18/pix2pixHD_slides.pptx">slides</a></strong> |<br>
        <strong><a href="https://github.com/NVIDIA/pix2pixHD">github</a></strong> |
        <strong><a href="https://goo.gl/YWuH8r">Youtube</a></strong> |                
        <strong><a href="javascript:toggleblock('pix2pixHD_cvpr_abs')">abstract</a></strong> |        
        <strong><a shape="rect" href="javascript:togglebib('pix2pixHD_cvpr')" class="togglebib">bibtex</a></strong> </p>
        <p align="justify"> <i id="pix2pixHD_cvpr_abs"> We present a new method for synthesizing high-resolution
        photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). 
        We generate 2048 Ã— 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and
        discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional
        features. First, we incorporate object instance segmentation information, which enables object manipulations such as 
        removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given 
        the same input, allowing users to edit the object appearance interactively. </i></p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2018pix2pixHD,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu 
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {High-Resolution Image Synthesis and Semantic Manipulation 
                with Conditional GANs},
   booktitle = {Proceedings of the IEEE Conference on 
                Computer Vision and Pattern Recognition (CVPR)},   
   year      = {2018},
}
      </pre> </div> </td> </tr>

      <tr> <td width="20%" valign="top"><a href="https://assets-production-webvanta-com.s3-us-west-2.amazonaws.com/000000/51/21/original/images/berkeley-logo.png" class="hoverZoomLink"><img src="https://assets-production-webvanta-com.s3-us-west-2.amazonaws.com/000000/51/21/original/images/berkeley-logo.png" alt="thesis" width="100%"></a>
        <td width="80%" valign="top"><p>
        <heading>Beyond Photo-Consistency: Shape, Reflectance, and Material Estimation Using<br> Light-Field Cameras</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Ting-Chun Wang</p>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-50.pdf">PhD Thesis</a></strong>, 2017</p>
        <p class="margin">&nbsp;</p>
      </td> </tr>

      <tr> <td width="20%" valign="top"><a href="papers/SIG17/lfv/teaser_highres.jpg" class="hoverZoomLink"><img src="papers/SIG17/lfv/teaser.jpg" alt="lfv_sig" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Light Field Video Capture Using a Learning-Based Hybrid Imaging System</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://nkhademi.com/">Nima Khademi Kalantari</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>ACM Transactions on Graphics (<strong>SIGGRAPH</strong>)</em>, 2017</p>
        <p class="margin">&nbsp;</p>
        
        <div class="paper" id="lfv_sig">    
        <p class="content">    
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video.pdf">paper</a></strong> |
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/paperData/LF_video_LoRes.pdf">lo-res pdf</a></strong> |      
        <strong><a href="javascript:toggleblock('lfv_sig_abs')">abstract</a></strong> |
        <strong><a href="https://www.youtube.com/watch?v=TqVKcssYfAo&feature=youtu.be">YouTube</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('lfv_sig')" class="togglebib">bibtex</a></strong> |        
        <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIG17/lfv/">project page</a></strong></p>
        <p align="justify"> <i id="lfv_sig_abs"> Capturing light fields requires a huge bandwidth to record the data: a modern light field camera can only take three images per second. Temporal interpolation at such extreme scale is infeasible as too much information will be entirely missing between adjacent frames. Instead, we develop a hybrid imaging system, adding another standard video camera to capture the temporal information. Given a 3 fps light field sequence and a standard 30 fps 2D video, our system can then generate a full light field video at 30 fps. We adopt a learning-based approach, which can be decomposed into two steps: spatio-temporal flow estimation and appearance estimation. The flow estimation propagates the angular information from the light field sequence to the 2D video, so we can warp input images to the target view. The appearance estimation then combines these warped images to output the final pixels. The whole process is trained end-to-end using convolutional neural networks.</i></p>
        <pre xml:space="preserve" style="display: none;">
@article{wang2017light,
   author  = {Ting-Chun Wang and Jun-Yan Zhu and Nima Khademi Kalantari 
              and Alexei A. Efros and Ravi Ramamoorthi},
   title   = {Light Field Video Capture Using a Learning-Based Hybrid 
              Imaging System},
   journal = {ACM Transactions on Graphics (Proceedings of SIGGRAPH)},
   volume  = {36},
   number  = {4},
   year    = {2017},
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="papers/PAMI17/model_highres.png" class="hoverZoomLink"><img src="papers/PAMI17/model.png" alt="brdf_pami" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p><heading>SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</heading><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2017</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="brdf_pami">
        <p class="content">
        <strong><a href="papers/PAMI17/BRDF_TPAMI.pdf">paper</a></strong> |
        <strong><a href="javascript:toggleblock('brdf_pami_abs')">abstract</a></strong> |
        <strong><a shape="rect" href="javascript:togglebib('brdf_pami')" class="togglebib">bibtex</a></p></strong>
        <p align="justify"> <i id="brdf_pami_abs">In this paper, we derive a spatially-varying (SV)BRDF-invariant theory for recovering 3D shape and reflectance from light-field cameras. Our key theoretical insight is a novel analysis of diffuse plus single-lobe SVBRDFs under a light-field setup. We show that, although direct shape recovery is not possible, an equation relating depths and normals can still be derived. Using this equation, we then propose using a polynomial (quadratic) shape prior to resolve the shape ambiguity. Once shape is estimated, we also recover the reflectance. We present extensive synthetic data on the entire MERL BRDF dataset, as well as a number of real examples to validate the theory, where we simultaneously recover shape and BRDFs from a single image taken with a Lytro Illum camera.  </i></p>                
        <pre xml:space="preserve" style="display: none;">
@article{wang2017svbrdf,
   title={{SVBRDF}-Invariant Shape and Reflectance 
   Estimation from Light-Field Cameras},
   author={Wang, Ting-Chun and Chandraker, Manmohan
   and Efros, Alexei and Ramamoorthi, Ravi},
   journal={IEEE Transactions on Pattern 
   Analysis and Machine Intelligence (TPAMI)},
   year={2017},
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/index_files/teaserSmall.png" class="hoverZoomLink"><img src="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/index_files/teaserSmall.png" alt="lfvs_sig" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Learning-Based View Synthesis for Light Field Cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://nkhademi.com/">Nima Khademi Kalantari</a>, <strong>Ting-Chun Wang</strong>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>ACM Transactions on Graphics (<strong>SIGGRAPH Asia</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>      
              
      <div class="paper" id="lfvs_sig">
      <p class="content">
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/PaperData/SIGGRAPHAsia16_ViewSynthesis.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('lfvs_sig_abs')">abstract</a></strong> |       
      <strong><a shape="rect" href="javascript:togglebib('lfvs_sig')" class="togglebib">bibtex</a></strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/">project page</a> </p> </strong>
      <p align="justify"> <i id="lfvs_sig_abs">With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase. </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@article{Kalantari16ViewSynthesis,
   author={Nima Khademi Kalantari and Ting-Chun Wang 
   and Ravi Ramamoorthi},
   title={Learning-Based View Synthesis for Light 
   Field Cameras},
   journal={ACM Transactions on Graphics (Proceedings 
   of SIGGRAPH Asia 2016)},
   year={2016},
}
      </pre> </div> </td> </tr>
      
      <tr> <td width="20%" valign="top"><a href="papers/ECCV16/lfmr_highres.png" class="hoverZoomLink"><img src="papers/ECCV16/lfmr.png" alt="lfmr_eccv" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>A 4D Light-Field Dataset and CNN Architectures for Material Recognition</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><b>Ting-Chun Wang</b>, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, Ebi Hiroaki, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>      
              
      <div class="paper" id="lfmr_eccv">
      <p class="content">
      <strong><a href="papers/ECCV16/LFMR.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('lfmr_eccv_abs')">abstract</a></strong> |   
      <strong><a href="papers/ECCV16/full_scene.html">HTML comparison</a></strong> |       
      <strong><a shape="rect" href="javascript:togglebib('lfmr_eccv')" class="togglebib">bibtex</a></strong> |
      <strong><a href="papers/ECCV16/dataset.html">dataset (2D thumbnail)</a></strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV16/LF_dataset.zip">full dataset (15.9G)</a> </p> </strong>
      <p align="justify"> <i id="lfmr_eccv_abs">We introduce a new light-field dataset of materials, and take advantage of the recent success of deep learning to perform material recognition on the 4D light-field. Our dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total. Since recognition networks have not been trained on 4D images before, we propose and compare several novel CNN architectures to train on light-field images. In our experiments, the best performing CNN architecture achieves a 7% boost compared with 2D image classification (70% to 77%). </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016dataset,
   title={A {4D} light-field dataset and {CNN} 
   architectures for material recognition},
   author={Wang, Ting-Chun and Zhu, Jun-Yan 
   and Hiroaki, Ebi and Chandraker, Manmohan 
   and Efros, Alexei and Ramamoorthi, Ravi},
   booktitle={Proceedings of European Conference on 
   Computer Vision (ECCV)},
   year={2016}
}
      </pre> </div> </td> </tr>
      
      <tr> <td width="20%" valign="top"><a href="papers/CVPR16/brdf_highres.png" class="hoverZoomLink"><img src="papers/CVPR16/brdf.png" alt="brdf_cvpr" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</a><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.nec-labs.com/~manu/">Manmohan Chandraker</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a><br></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2016
      <font color="red">(oral presentation)</font></p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="brdf_cvpr">
      <p class="content">
      <strong><a href="papers/CVPR16/LF_BRDF.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('brdf_cvpr_abs')">abstract</a></strong> |   
      <strong><a href="papers/CVPR16/supplementary_document.pdf">supplementary</a></strong> |   
      <strong><a href="papers/CVPR16/MERL.html">HTML comparison</a></strong> |                 
      <strong><a shape="rect" href="javascript:togglebib('brdf_cvpr')" class="togglebib">bibtex</a></p></strong>
      <p align="justify"> <i id="brdf_cvpr_abs">In this paper, we derive a spatially-varying (SV)BRDF-invariant theory for recovering 3D shape and reflectance from light-field cameras. Our key theoretical insight is a novel analysis of diffuse plus single-lobe SVBRDFs under a light-field setup. We show that, although direct shape recovery is not possible, an equation relating depths and normals can still be derived. Using this equation, we then propose using a polynomial (quadratic) shape prior to resolve the shape ambiguity. Once shape is estimated, we also recover the reflectance. We present extensive synthetic data on the entire MERL BRDF dataset, as well as a number of real examples to validate the theory, where we simultaneously recover shape and BRDFs from a single image taken with a Lytro Illum camera.  </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016svbrdf,
   title={SVBRDF-invariant shape and reflectance 
   estimation from light-field cameras},
   author={Wang, Ting-Chun and Chandraker, Manmohan 
   and Efros, Alexei and Ramamoorthi, Ravi},
   booktitle={Proceedings of the IEEE Conference on 
   Computer Vision and Pattern Recognition (CVPR)},
   year={2016}
}
      </pre> </div> </td> </tr> 
      
      <tr> <td width="20%" valign="top"><a href="papers/CVPR16/stereo_highres.png" class="hoverZoomLink"><img src="papers/CVPR16/stereo.png" alt="stereo_cvpr" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth from Semi-Calibrated Stereo and Defocus</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.msrik.com/">Manohar Srikanth</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2016</p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="stereo_cvpr">
      <p class="content">
      <strong><a href="papers/CVPR16/semi_stereo.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('stereo_cvpr_abs')">abstract</a></strong> |  
      <strong><a href="papers/CVPR16/index.html">HTML comparison</a></strong> |                      
      <strong><a shape="rect" href="javascript:togglebib('stereo_cvpr')" class="togglebib">bibtex</a></p></strong>
      <p align="justify"> <i id="stereo_cvpr_abs">In this work, we propose a multi-camera system where we combine a main high-quality camera with two low-res auxiliary cameras. The auxiliary cameras are well calibrated and act as a passive depth sensor by generating disparity maps. The main camera has an interchangeable lens and can produce good quality images at high resolution. Our goal is, given the low-res depth map from the auxiliary cameras, generate a depth map from the viewpoint of the main camera. The advantage of our system, compared to other systems such as light-field cameras or RGBD sensors, is the ability to generate a high-resolution color image with a complete depth map, without sacrificing resolution and with minimal auxiliary hardware.  </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2016semi,
   title={Depth from semi-calibrated stereo and defocus},
   author={Wang, Ting-Chun and Srikanth, Manohar
   and Ramamoorthi, Ravi},
   booktitle={Proceedings of the IEEE Conference on 
   Computer Vision and Pattern Recognition (CVPR)},
   year={2016}
}
      </pre> </div> </td> </tr> 
      
    <tr> <td width="20%" valign="top"><a href="papers/PAMI16/model_highres.png" class="hoverZoomLink"><img src="papers/PAMI16/model.png" alt="occlusion_pami" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth Estimation with Occlusion Modeling Using Light-field Cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2016 </p>      
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="occlusion_pami">
      <p class="content">
      <strong><a href="papers/PAMI16/occlusion_TPAMI.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('occlusion_pami_abs')">abstract</a></strong> |
      <strong><a shape="rect" href="javascript:togglebib('occlusion_pami')" class="togglebib">bibtex</a></p></strong>
      <p align="justify"> <i id="occlusion_pami_abs">In this paper, an occlusion-aware depth estimation algorithm is developed; the method also enables identification of occlusion edges,which may be useful in other applications. It can be shown that although photo-consistency is not preserved for pixels at occlusions, it still holds in approximately half the viewpoints. Moreover, the line separating the two view regions (occluded object vs. occluder) has the same orientation as that of the occlusion edge in the spatial domain. By ensuring photo-consistency in only the occluded view region, depth estimation can be improved.  </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@article{wang2016depth,
   title={Depth estimation with occlusion modeling 
   using light-field cameras},
   author={Wang, Ting-Chun and Efros, Alexei and 
   Ramamoorthi, Ravi},
   journal={IEEE Transactions on Pattern 
   Analysis and Machine Intelligence (TPAMI)},
   volume={38},
   number={11},
   pages={2170--2181},
   year={2016},
}
      </pre> </div> </td> </tr> 
      
      <tr>
      <td width="20%" valign="top"><a href="papers/ICCV15/teaser_highres.jpg" class="hoverZoomLink"><img src="papers/ICCV15/teaser.jpg" alt="occlusion" width="100%" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Occlusion-aware depth estimation using light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><strong>Ting-Chun Wang</strong>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei Efros</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2015 </p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="occlusion">
      <p class="content">
      <strong><a href="papers/ICCV15/LF_occlusion_ICCV15.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('occlusion_abs')">abstract</a></strong> |
      <strong><a href="papers/ICCV15/supplementary.html">supplementary</a></strong> | 
      <strong><a shape="rect" href="javascript:togglebib('occlusion')" class="togglebib">bibtex</a></strong> |
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ICCV15/occCode.zip">code</a></strong> | 
      <strong><a href="http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ICCV15/dataset.zip">dataset (3.3GB)</a></p></strong>
      <p align="justify"> <i id="occlusion_abs">In this paper, we develop a depth estimation algorithm for light field cameras that treats occlusion explicitly; the method also enables identification of occlusion edges, which may be useful in other applications. We show that, although pixels at occlusions do not preserve photo-consistency in general, they are still consistent in approximately half the viewpoints. </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@inproceedings{wang2015occlusion,
  title={Occlusion-aware depth estimation 
  using light-field cameras.},
  author={Wang, Ting-Chun and 
  Efros, Alexei and Ramamoorthi, Ravi},
  booktitle={Proceedings of the IEEE International 
  Conference on Computer Vision (ICCV)},
  year={2015}
}
      </pre> </div> </td> </tr> 
            
      
      <tr>
      <td width="20%" valign="top"><a href="papers/PAMI15/specular_highres.png" class="hoverZoomLink"><img src="papers/PAMI15/specular.png" alt="glossy_pami" width="100%"  height="150" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth estimation and specular removal for glossy surfaces using <br>
          point and line consistency with light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://www.cs.berkeley.edu/~mtao/">Michael Tao</a>, Jong-Chyi Su, <strong>Ting-Chun Wang</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em></p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="glossy_pami">
      <p class="content">
      <strong><a href="papers/PAMI15/LF_glossy_TPAMI.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('glossy_pami_abs')">abstract</a></strong> |
      <strong><a shape="rect" href="javascript:togglebib('glossy_pami')" class="togglebib">bibtex</a> </p></strong>
      <p align="justify"> <i id="glossy_pami_abs">Light-field cameras have now become available in both consumer and industrial applications, and recent papers have demonstrated practical algorithms for depth recovery from a passive single-shot capture. However, current light-field depth estimation methods are designed for Lambertian objects and fail or degrade for glossy or specular surfaces. In this paper, we present a novel theory of the relationship between light-field data and reflectance from the dichromatic model. </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@article{tao2015depth,
title={Depth Estimation and Specular Removal 
for Glossy Surfaces Using Point and 
Line Consistency with Light-Field Cameras},
author={Tao, Michael and Su, Jong-Chyi 
and Wang, Ting-Chun and Malik, Jitendra 
and Ramamoorthi, },
journal={IEEE Transactions on Pattern 
Analysis \& Machine Intelligence},
number={1},
pages={1--1},
year={2015},
publisher={IEEE}
}
      </pre></div></td></tr>
      
      
      <tr>
      <td width="20%" valign="top"><a href="papers/ECCV14/l4cv_highres.png" class="hoverZoomLink"><img src="papers/ECCV14/l4cv.jpg" alt="glossy" width="100%" height="150" border="1"></a>
      <td width="80%" valign="top">
      <p><heading>Depth estimation for glossy surfaces with light-field cameras</heading><br>
      <p class="margin">&nbsp;</p>
      <p class="content"><a href="http://www.cs.berkeley.edu/~mtao/">Michael Tao</a>, <strong>Ting-Chun Wang</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a></p>
      <p class="margin-small">&nbsp;</p>
      <p class="content"><em>ECCV workshop on Light Fields for Computer Vision (<strong>L4CV</strong>)</em>, 2014</p>
      <p class="margin">&nbsp;</p>
              
      <div class="paper" id="glossy">
      <p class="content">
      <strong><a href="papers/ECCV14/LF_glossy_ECCV14_s.pdf">paper</a></strong> |
      <strong><a href="javascript:toggleblock('glossy_abs')">abstract</a></strong> |
      <strong><a shape="rect" href="javascript:togglebib('glossy')" class="togglebib">bibtex</a> </p> </strong>
      <p align="justify"> <i id="glossy_abs">Light-field cameras have now become available in both consumer and industrial applications, and recent papers have demonstrated practical algorithms for depth recovery from a passive single-shot capture. In this paper, we develop an iterative approach to use the benefits of light-field data to estimate and remove the specular component, improving the depth estimation. The approach enables light-field data depth estimation to support both specular and diffuse scenes. </i></p>
              
      <pre xml:space="preserve" style="display: none;">
@inproceedings{tao2014depth,
title={Depth estimation for glossy 
surfaces with light-field cameras},
author={Tao, Michael W and Wang, Ting-Chun 
and Malik, Jitendra and Ramamoorthi, Ravi},
booktitle={Computer Vision-ECCV 2014 Workshops},
pages={533--547},
year={2014},
organization={Springer}
}
      </pre> </div> </td> </tr>    
      </table>
  </div>
  </div>
    </div>
    <div id="footer">
      
</div>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('pix2pixHD_cvpr_abs');
hideblock('lfv_sig_abs');
hideblock('brdf_pami_abs');
hideblock('lfvs_sig_abs');
hideblock('lfmr_eccv_abs');
hideblock('brdf_cvpr_abs');
hideblock('stereo_cvpr_abs');
hideblock('occlusion_pami_abs');
hideblock('occlusion_abs');
hideblock('glossy_pami_abs');
hideblock('glossy_abs');
</script>


<br>
<p class="title-large">Software</p>

<p class="content"><strong><a href="https://github.com/junyanz/light-field-video">Light Field Video</a></strong>: Light field video applications (e.g. video refocusing, changing aperture and view).</p>
<br>



<p class="title-large">Talks</p>
<p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaVkpxcVpIOGhfeTQ">Beyond Photo-Consistency: Shape, Reflectance, and Material Estimation Using Light-Field Cameras</a></strong></p>
<p class="content">Dissertation talk (2017)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaQU9DeXJDRG81ZXc">Light Field Video Capture Using a Learning-Based Hybrid Imaging System</a></strong></p>
<p class="content">Siggraph (2017)</p>
<p class="margin">&nbsp;</p>

<p class="content"><strong><a href="https://drive.google.com/open?id=0BzhS93j2LkNaMHpUVkNpM2JWbms">SVBRDF-Invariant Shape and Reflectance Estimation from Light-Field Cameras</a></strong></p>
<p class="content">CVPR (2016)</p>

<br>


<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="./files/counter.js.download"></script><script language="javascript">var _go_js="1.0";</script><script language="javascript1.1">_go_js="1.1";</script><script language="javascript1.2">_go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script><script language="javascript1.4">_go_js="1.4";</script><script language="javascript1.5">_go_js="1.5";</script><script language="javascript1.6">_go_js="1.6";</script><script language="javascript1.7">_go_js="1.7";</script><script language="javascript1.8">_go_js="1.8";</script><script language="javascript1.9">_go_js="1.9";</script><script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
<!-- <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> -->
</div>
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body></html>